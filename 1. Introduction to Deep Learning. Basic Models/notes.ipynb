{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Day 1\n",
    "\n",
    "## Introduction and Simple Models\n",
    "\n",
    "### Deep Learning vs. Classic Machine Learning\n",
    "1. **Classification and Regression**: Deep Learning excels in both tasks, especially when dealing with large, complex datasets.\n",
    "2. **Using Big Data**: While classical ML often struggles with large datasets, Deep Learning thrives by leveraging massive amounts of data to improve performance.\n",
    "\n",
    "### Emerging Topics\n",
    "- **Large Language Models (LLMs)**: Powerful for understanding and generating human-like text.\n",
    "- **Generative Models**: Used for creating new content, such as images, text, or audio.\n",
    "- **Reinforcement Learning (RL)**: Focuses on learning optimal policies through interaction with an environment.\n",
    "\n",
    "## Frameworks\n",
    "1. **Scikit-Learn**: Often used for simpler ML models.\n",
    "2. **TensorFlow**: Widely adopted for production applications.\n",
    "3. **PyTorch**: Favored in research, especially by organizations like Hugging Face.\n",
    "\n",
    "## Curriculum Overview\n",
    "1. **Introduction and Basic Models**\n",
    "2. **Training and Improving Neural Networks (NNs)**\n",
    "3. **Neural Networks for Image Processing**\n",
    "4. **Neural Networks for Language Processing**\n",
    "5. **Advanced Neural Network Architectures**\n",
    "6. **Generative Models**\n",
    "7. **Reinforcement Learning**\n",
    "\n",
    "## Notes on Tools and Training\n",
    "- **TensorFlow**: Predominantly used in production settings.\n",
    "- **PyTorch**: The go-to choice for research.\n",
    "- **Reinforcement Learning from Human Feedback (RLHF)**: A critical area in modern RL applications.\n",
    "\n",
    "### Key Dates\n",
    "- **Regular Exam**: 15–16 February 2025  \n",
    "  (Project and summaries deadline: 13 February 2025).\n",
    "- **Retake Exam**: 1–2 March 2025.\n",
    "- **Homework Assignments**: Provided regularly.\n",
    "\n",
    "### Exam Structure\n",
    "1. **Theory**: 10 questions in 30 minutes (30% of grade).\n",
    "2. **Practice**: Two article summaries (application-focused and theory/concept-focused) worth 20%, and a project worth 80%.\n",
    "\n",
    "## Introduction to Deep Learning\n",
    "\n",
    "### Basic Models: Advantages and Disadvantages\n",
    "- **Advantages**:  \n",
    "  - Utilizes GPUs and specialized hardware.  \n",
    "  - Handles larger datasets and more parameters.  \n",
    "  - Reduces the need for complex feature selection.  \n",
    "  - Supports multi-output models.\n",
    "- **Disadvantages**:  \n",
    "  - High computational cost.  \n",
    "  - Not necessary for small datasets or simple tasks (\"If your model trains in less than a day, you don’t need a Deep Learning model\").\n",
    "\n",
    "## Introduction to Neural Networks (NNs)\n",
    "1. **Computational Graphs**: Represent operations and data flow.\n",
    "2. **Directed Acyclic Graphs (DAGs)**: Ensure no cycles in computations.\n",
    "3. **Feedforward Neural Networks**: The simplest NN architecture.\n",
    "4. **Network Structure**: Input > Input layer > Hidden layer(s) (≥1) > Output layer > Output.\n",
    "5. **Key Features**: No links between units within the same layer.\n",
    "6. **Setup**: Install TensorFlow, PyTorch, and Nvidia CUDA.\n",
    "7. **AST (Abstract Syntax Tree)**: Useful for programmatic NN construction.\n",
    "8. **Propagation**: Includes Forward and Backward Propagation.\n",
    "9. **Tensors**: Scalars, vectors, matrices, and higher-order tensors (e.g., RGB channels).\n",
    "\n",
    "## Framework-Specific Notes\n",
    "\n",
    "### TensorFlow\n",
    "1. **High-Level API**: TensorFlow’s primary high-level API is Keras.\n",
    "2. **Components**: Import `Sequential`, `Input`, and `Dense` from `tf.keras`.\n",
    "3. **Model Summary**: Provides details about layers and parameters.\n",
    "4. **Loss Function**: Use `sparse_categorical_crossentropy` for classification tasks in `tf.compile`.\n",
    "5. **Activation Function**: Use `softmax` for multi-class classification.\n",
    "6. **Optimizer**: Typically, `adam`.\n",
    "7. **Metrics**: Commonly track `accuracy`.\n",
    "8. **Training**: Via `model.fit`.\n",
    "9. **Epochs**: Define the number of forward and backward passes.\n",
    "10. **Batch Size**: Determines the subset of data used per update.\n",
    "11. **Session Management**: Use `keras.clear_session()` to manage memory.\n",
    "\n",
    "### PyTorch\n",
    "1. **API**: Object-oriented and modular.\n",
    "2. **Tensors**: Convert data into tensors for computation.\n",
    "3. **Loss Function**: Defined as a criterion.\n",
    "4. **Optimization**: Managed through optimizers.\n",
    "5. **Training Loop**: Conducted within a `for` loop.\n",
    "6. **Gradient Updates**: Use `optimizer.zero_grad()` before backpropagation.\n",
    "7. **Extensions**: Incorporates `PyTorch Lightning` for streamlined training.\n",
    "8. **Evaluation**: Use `torcheval` for performance metrics.\n",
    "\n",
    "### Notes\n",
    "- GPUs can accelerate training by up to 104x compared to CPUs.\n",
    "- Install `Lightning` and `torcheval` for enhanced functionality.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
