{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deep Learning Day 2**\n",
    "\n",
    "### Administrative Notes\n",
    "1. **Next Lecture**: 2nd January 2025.  \n",
    "2. **Public Defense**: Scheduled for 10th February 2025. Ensure submission before 9th February 2025.\n",
    "\n",
    "### Extra Information\n",
    "1. **Handling Large Datasets**: If a dataset does not fit in RAM, use methods like `partial_fit` to train the model incrementally.\n",
    "2. **Gradient Descent Alternatives**: Consider methods like Kolmogorov-Arnold or Simulated Annealing for optimization tasks.\n",
    "3. **Neural Network Layers**: A layer without an activation function behaves as Linear Regression.\n",
    "4. **Multithreading in Data Loading**: The `num_workers` parameter specifies the number of CPU cores utilized for parallel processing.\n",
    "5. **Activation Functions**:  \n",
    "   - **ReLU**: Rectified Linear Unit, commonly used for introducing non-linearity.  \n",
    "   - **Clamped Activation Function**: A variation of ReLU with an upper limit.\n",
    "6. **Initial Model Fitting**: Start by fitting the model on a small batch (e.g., 8 records). If it does not overfit, investigate potential coding errors.\n",
    "7. **Model Saving/Loading**: Similar to `pickle`, save or load models to/from a file for reuse.\n",
    "\n",
    "### Building Models: TensorFlow (TF) vs. PyTorch\n",
    "1. **Object-Oriented Programming (OOP)**: A common approach for model-building in both frameworks.\n",
    "2. **TensorFlow Functional API**:  \n",
    "   - Supports multiple input/output configurations.  \n",
    "   - Enables variable reuse and flexible combination of variables.\n",
    "3. **PyTorch Models**: Similar principles apply, with layers and models defined using OOP.\n",
    "4. **Data Pipelines**:  \n",
    "   - **TensorFlow**: Use `tf.data.Dataset` for efficient data handling.  \n",
    "   - **PyTorch**: Leverage `Dataset` and `DataLoader` for sequential data processing.\n",
    "5. **Tutorials**:  \n",
    "   - [TensorFlow Data Performance Guide](https://www.tensorflow.org/guide/data_performance).  \n",
    "   - [PyTorch Data Pipeline Tutorial](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html).\n",
    "\n",
    "### Bias-Variance Tradeoff and Error Analysis\n",
    "1. **Regularization**:  \n",
    "   - **L1 Regularization**: Reduces weights, potentially turning some to zero.  \n",
    "   - **L2 Regularization**: Penalizes large weights.  \n",
    "   - Includes kernel, bias, and activity regularizers.\n",
    "2. **Dropout**:  \n",
    "   - Randomly deactivates units in a layer during training.  \n",
    "   - Specify the dropout rate as the percentage of units to deactivate.  \n",
    "   - Note: Do not use dropout during inference.  \n",
    "   - When applied just after the input layer, it acts like \"feature selection\" or \"data denoising\" but is not recommended for this purpose.\n",
    "3. **Data Splitting**:  \n",
    "   - Splits such as 70/30 or 80/20 are guidelines, not strict rules.  \n",
    "   - For large datasets (e.g., 1M rows): Use 980K for training, 10K for validation, and 10K for testing.\n",
    "4. **Bias-Variance Error Analysis**:  \n",
    "   - **High Bias**: Address with more data, additional features, or a more complex model.  \n",
    "   - **High Variance**: Mitigate with simpler models, regularization, or improved data quality.\n",
    "5. **Training/Validation Curves**: Use tools like TensorBoard (`%load_ext tensorboard`) and callbacks (e.g., `ModelCheckpoint`) for monitoring.\n",
    "\n",
    "### Optimization\n",
    "#### I. Vanishing/Exploding Gradients\n",
    "1. Use appropriate weight initialization techniques (e.g., Xavier/Glorot initialization).\n",
    "2. Leverage mini-batch gradient descent and `partial_fit` to handle these issues.\n",
    "\n",
    "#### II. Advanced Optimizers\n",
    "1. **Momentum**: Uses exponential moving averages to accelerate gradient descent.\n",
    "2. **RMSprop**: Adjusts learning rates using root mean square propagation.\n",
    "3. **Adam**: Combines momentum and RMSprop for adaptive learning.\n",
    "\n",
    "#### III. Hyperparameter Tuning\n",
    "1. Activation functions are often the last hyperparameter to tune (default to ReLU unless issues arise).\n",
    "2. Grid search is limited for large search spaces; prefer random or Bayesian search techniques.\n",
    "3. Examples of using Optuna for hyperparameter optimization:  \n",
    "   - [TensorFlow Example](https://github.com/optuna/optuna-examples/blob/main/tensorflow/tensorflow_eager_simple.py).  \n",
    "   - [PyTorch Example](https://github.com/optuna/optuna-examples/blob/main/pytorch/pytorch_simple.py).\n",
    "4. Key hyperparameters to tune:  \n",
    "   - Learning rate.  \n",
    "   - Number of hidden units.  \n",
    "   - Number of hidden layers.  \n",
    "   - Momentum term.  \n",
    "   - Mini-batch size.\n",
    "\n",
    "#### IV. Batch Normalization\n",
    "1. Normalize inputs using techniques like Z-score normalization.\n",
    "2. Apply batch normalization to standardize activations of the prior layer.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
